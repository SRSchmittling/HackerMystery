# Ethical Hacker's Manual

Computers really only understand 1's and 0's where 1 represents the presence of voltage (think of a light-switch being turned on) and a 0 represents no voltage (the light-switch is off). While programming languages look readable to humans, in order for the computer to use them, they have to be translated into "machine code", a binary represeentation of the instructions (1's and 0's). 

## Machine Code
How does machine code work? 
